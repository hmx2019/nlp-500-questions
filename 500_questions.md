1 为什么用余弦相似度表示的词之间的差异显著高于0.5？


2 word2vec与lda有什么区别？

  首先，LDA是按照文档中单词的共现关系来对单词按照主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”两个概率分布。而word2vec实际上是对“上下文-单词”矩阵进行学习，其中上下文由周围几个单词组成，由此学到的词向量更多融入了上下文特征。
  主题模型和词嵌入两类方法最大的不同在于模型本身。
  主题模型是一种基于概率图模型的生成式模型。其似然函数可以写为若干条件概率连乘的形式，其中包含需要推测的隐含变量(即主题)
  词嵌入模型一般表示为神经网络的形式，似然函数定义在网络的输出之上。需要学习网络的权重来得到单词的稠密向量表示。

3 处理文本数据时， tf-idf、rnn、cnn有什么区别和特点？

  传统文本处理任务的方法一般将TF-IDF向量作为特征输入，这样实际上丢失了输入的文本系列中每个单词的顺序。
  CNN一般会接收一个定长的向量作为输入，然后通过滑动窗口加池化的方法将原来的输入转换为一个固定长度的向量表示。这样做可以捕捉到文本中的一些局部特征，但是两个单词之间的长距离依赖关系难以学习。
  RNN能够很好处理文本数据变长并且有序的输入序列。将前面阅读到的有用信息编码到状态变量中去，从而拥有了一定的记忆能力。
![image](https://github.com/hmx2019/nlp-500-questions/blob/master/img/q3-01.jpg)

4 Seq2Seq模型加入注意力机制是为了解决什么问题？为什么选用了双向循环神经网络？
 
5 tf-idf 有什么缺点？

  一、本质上，tf-idf是一种对噪音量化并加权的措施，其认为某个词在语言中出现的频率与其重要性成反比，  在本质上IDF是一种试图抑制噪音的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用。这对于大部分文本信息，并不是完全正确的。IDF的简单结构并不能使提取的关键词，十分有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能。尤其是在同类语料库中，这一方法有很大弊端，往往一些同类文本的关键词被掩盖。例如：语料库D中教育类文章偏多，而文本j是一篇属于教育类的文章，那么教育类相关的词语的IDF值将会偏小，使提取文本关键词的召回率更低。
  TF-IDF的优点是实现简单，相对容易理解。但是，TFIDF算法提取关键词的缺点也很明显，严重依赖语料库，需要选取质量较高且和所处理文本相符的语料库进行训练。另外，对于IDF来说，它本身是一种试图抑制噪声的加权，本身倾向于文本中频率小的词，这使得TF-IDF算法的精度不高。
 
   二、TF-IDF算法还有一个缺点就是不能反应词的位置信息，在对关键词进行提取的时候，词的位置信息，例如文本的标题、文本的首句和尾句等含有较重要的信息，应该赋予较高的权重。

6 bert模型可以做相似度任务吗？

  一.首先一点是在不finetune的情况下，cosine similairty绝对值没有实际意义，bert pretrain计算的cosine similairty都是很大的，如果你直接以cosine similariy>0.5之类的阈值来判断相似不相似那肯定效果很差。如果用做排序，也就是cosine(a,b)>cosine(a,c)->b相较于c和a更相似，是可以用的。总而言之就是你模型评价的标准应该使用auc，而不是accuracy

  二.短文本（新闻标题）语义相似度任务用先进的word embedding（英文fasttext/glove，中文tencent embedding）mean pooling后的效果就已经不错；而对于长文本（文章）用simhash这种纯词频统计的完全没语言模型的简单方法也ok

  三.bert pretrain模型直接拿来用作 sentence embedding效果甚至不如word embedding，cls的emebdding效果最差（也就是你说的pooled output）。把所有普通token embedding做pooling勉强能用（这个也是开源项目bert-as-service的默认做法），但也不会比word embedding更好。

  四.用siamese的方式训练bert，上层通过cosine做判别，能够让bert学习到一种适用于cosine作为最终相似度判别的sentence embedding，效果优于word embedding，但因为缺少sentence pair之间的特征交互，比原始bert sentence pair fine tune还是要差些。


7 bert pretrain 计算的cosine similarity 为什么都很大？冰箱和妈妈居然大于0.95


